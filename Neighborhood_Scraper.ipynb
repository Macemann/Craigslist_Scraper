{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "CH_url = \"https://washingtondc.craigslist.org/search/apa?query=Columbia%20Heights&s=0\"\n",
    "AM_url = \"https://washingtondc.craigslist.org/search/apa?query=Adams%20Morgan&s=0\"\n",
    "\n",
    "def scraper(url):\n",
    "    r = requests.get(url)\n",
    "\n",
    "    soup = BeautifulSoup(r.content, \"lxml\")\n",
    "    #print soup\n",
    "\n",
    "    price_data = []\n",
    "    data1 = soup.find_all(\"span\", {\"class\":\"price\"})\n",
    "    for price in data1:\n",
    "        price_data.append(price.contents[0])\n",
    "    \n",
    "    neighborhood_data = []\n",
    "    data2 = soup.find_all(\"span\", {\"class\": \"pnr\"})\n",
    "    for neighborhood in data2:\n",
    "        neighborhood_data.append(neighborhood.contents[1].contents[0])\n",
    "\n",
    "    size_data = []\n",
    "    data3 = soup.find_all(\"span\", {\"class\": \"housing\"})\n",
    "    for size in data3:\n",
    "        size_data.append(size.contents[0])\n",
    "\n",
    "    date_data = []\n",
    "    data4 = soup.find_all(\"span\", {\"class\": \"pl\"})\n",
    "    for date in data4:\n",
    "        date_data.append(date.contents[1].contents[0])\n",
    "\n",
    "    comment_data = []\n",
    "    data5 = soup.find_all(\"span\", {\"class\": \"pl\"})\n",
    "    for comment in data5:\n",
    "        comment_data.append(comment.contents[3].contents[0])\n",
    "\n",
    "    link_data = []\n",
    "    data6 = soup.find_all(\"span\", {\"class\": \"pl\"})\n",
    "    for link in data6:\n",
    "        baselink = url[:35]\n",
    "        link = re.sub(r'.*(/doc.*)\\.html.*', baselink + r'\\1'+ r'.html', str(link))\n",
    "        link_data.append(link)\n",
    "\n",
    "    id_data = []\n",
    "    data7 = soup.find_all(\"span\", {\"class\": \"pl\"})\n",
    "    for ids in data7:\n",
    "        ids = re.sub(r'.*data-id=\"(.*[0-9])\\\".*', r'\\1', str(ids))\n",
    "        id_data.append(ids)\n",
    "    \n",
    "    data = zip(price_data,neighborhood_data,size_data,date_data,comment_data,link_data,id_data)\n",
    "    #print data\n",
    "    \n",
    "    with open(url[53:58]+\"_scraper_output.csv\",\"wb\") as output:\n",
    "        csv_out=csv.writer(output)\n",
    "        csv_out.writerow([\"Price\",\"Neighborhood\",\"Size\", \"Date Posted\", \"Comment\", \"Link\", \"ID\"])\n",
    "        for row in data:\n",
    "            csv_out.writerow(row)\n",
    "\n",
    "scraper(CH_url)\n",
    "scraper(AM_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef clean_row(dirty_row):\\n    stripped_row = [data.strip() for data in dirty_row]\\n    price_data,neighborhood_data,size_data,date_data,comment_data,link_data = stripped_row\\n\\n    neighborhood_data = re.sub(r\\'\\\\((.+?)\\\\)\\', r\\'\\x01\\', neighborhood_data)\\n    size_data = re.sub(r\\'.*?([0-9])+br\\\\s*-\\\\s*([0-9]*).+\\', r\\'\\x01, \\x02\\', size_data)\\n    #date_data = re.sub(r\\'\\')\\n    #comment_data = re.sub(r\\'\\')\\n    baselink = url[:34]\\n    link_data = re.sub(r\\'\"(/doc+.*)\"\\', r\\'\\x01\\', baselink+link_data)\\n\\n    return neighborhood_data\\n\\n\\ndef clean(dirty_rows):\\n    # clean_rows = [clean(row) row for row in dirty_rows]\\n    clean_rows = []\\n    for row in dirty_rows:\\n        clean_rows.append(clean_row(row))\\n    return clean_rows\\n\\ndirty = []\\n\\nwith open(\"scraper_output.csv\", \"rb\") as f:\\n    reader = csv.reader(f)\\n    for row in reader:\\n        dirty.append(row)\\nprint clean(dirty)\\n'"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "time_data = []\n",
    "data8 = time_data.append([time])\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "def clean_row(dirty_row):\n",
    "    stripped_row = [data.strip() for data in dirty_row]\n",
    "    price_data,neighborhood_data,size_data,date_data,comment_data,link_data = stripped_row\n",
    "\n",
    "    neighborhood_data = re.sub(r'\\((.+?)\\)', r'\\1', neighborhood_data)\n",
    "    size_data = re.sub(r'.*?([0-9])+br\\s*-\\s*([0-9]*).+', r'\\1, \\2', size_data)\n",
    "    #date_data = re.sub(r'')\n",
    "    #comment_data = re.sub(r'')\n",
    "    baselink = url[:34]\n",
    "    link_data = re.sub(r'\"(/doc+.*)\"', r'\\1', baselink+link_data)\n",
    "\n",
    "    return neighborhood_data\n",
    "\n",
    "\n",
    "def clean(dirty_rows):\n",
    "    # clean_rows = [clean(row) row for row in dirty_rows]\n",
    "    clean_rows = []\n",
    "    for row in dirty_rows:\n",
    "        clean_rows.append(clean_row(row))\n",
    "    return clean_rows\n",
    "\n",
    "dirty = []\n",
    "\n",
    "with open(\"scraper_output.csv\", \"rb\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        dirty.append(row)\n",
    "print clean(dirty)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
